[{"content":"Module Introduction For this module we had to compare different AI techniques. Which those techniques are was up to us. During the semester we studied Finite State Machines, Rule Based Systems, Fuzzy Logic, Reinforcement and Predictive Learning, Genetic Algorithms, Case Based Reasoning, Artificial Neural Networks, Self-Organaing Maps and Clustering and Deep Neural Networks. I decided to compare 3 different techniques from Machine Learning - Reinforcement Learning and 2 types of Imitation Learning - Behavioral Cloning and GAIL (Generative Adversarial Imitation Learning)\nAI in games Video Games have been using different Artificial Intelligence techniques since the very beginning. Although some of those techniques might not necessarily be classified as “intelligent” in 2021. Artificial Intelligence in games could be as simple as calculating where the paddle in “Pong” should be, deciding the state and the behaviour of the ghosts in “Pac-Man” or beating the very best players in one of the most complex style of games we have to date, using Artificial Neural Networks (OpenAI) - “Dota 2”.\nThe goals of Games AI are often very different compared to the goals of Academic AI. In games AI serves the purpose to enhance the experience of the player as well as providing better entertainment or immersion. It does not focus on being better than the player and finding a balance between how strong the AI should be is often difficult. Another limiting factor in Games AI is computational power, often the computers of the players are far from the best in class, thus the Game AI is made to use much less resources than Academic AI.\nAn example of common Games AI techniques are Finite State Machines (FSM). One of the earliest uses of an FSM is the game “Pac-Man”. Each Ghost in the game has the same “Evade” state, which is triggered when the player interacts with a pill. Each Ghost also has its own implementation of the “Chase” state, giving each ghost a unique character. Transitioning from Evade to Chase happens on a timer, which when expires, the state changes. FSM’s are still commonly used in modern games for Non Playable Characters (NPC’s), a simple example is a guard who has a couple of states - Patrolling, Alert, Engaging.\nProject Introduction This application demonstrates a different AI technique, Machine Learning. More precisely how three different types of machine learning compare against each other in a semi-complex example built in Unity. The runnable application includes the trained brains showcasing the respective results of the testing. The ML library used was the ML-Agents package from Unity. The three ML algorithms used were Reinforcement Learning, GAIL (Generative Adversarial Imitation Learning) and Behavioral Cloning. The test scenario in the application is a small environment which contains a player (who has an ML agent), a button/trigger, and a goal which is created when the button is pressed. A full test loop includes the player being created at a random point in the environment, the button being created in another random point, the player agent navigating successfully to the location of the button, interacting with it in order to spawn the goal, then locating the goal and the player agent reaching the goal. The inputs for the ML agent are using the unity input system. The inputs are the player movement on the Horizontal and Vertical axis and an interaction key (“E”) to use the button and spawn the goal.\nThis AI technique was chosen due to the vast potential it has for future implementation in Video Games. It was chosen to gain a deeper understanding of how ML works and how it could be used to train AI. The Test environment was selected in order to showcase strengths and weaknesses of the different algorithms.\nML background Machine Learning is a type of Artificial Intelligence that uses data to train and make more accurate predictions. There are three main ML algorithms - Unsupervised learning, Supervised Learning and Reinforcement Learning. Unsupervised Learning is aimed at helping identify patterns in data. It also requires to be fed the data wanted to be analysed beforehand and unlike Supervised, that data does not contain correct examples inside. It must analyze and categorize the data on its own. Supervised Learning, as the name suggests, has a supervisor as the teacher. That means that some of the data that is fed into the algorithm already contains the correct answer, it then uses that training data as an example and produces a correct outcome from the training data. Reinforcement Learning on the other hand, does not require input data beforehand. It relies on gathering the information from the environment by using different sensors, making observations and doing actions based on them. The final piece of reinforcement learning is the reward signal, the agent is trained to maximize its overall rewards, it is up to the user of the ML to define those rewards, negative and/or positive.\nThe test application utilized the Single-Agent and Simultaneous Single-Agent training models. A Single-Agent model is the traditional method of training an agent, a single environment and a single agent. The Simultaneous Single-Agent is a logical step up, again its one agent per environment, but there are multiple instances of them. Essentially parallelization of the training, which can speed up and stabilize the training process. After all, training 100 identical agents simultaneously is faster than training a single instance.\nThe underlying Reinforcement algorithm used by Unity’s Machine Learning toolkit is called Proximal Policy Optimization (PPO). This is a method that has been shown to be more general purpose and stable than many other Reinforced Learning algorithms. It is lighter than competing algorithms such as TRPO and ACER. It accomplishes that by a formula which better balances the policy grading by trying to compute an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relatively small.\nThere are two different Imitation Learning (IL) algorithms showcased by the application. Imitation learning could be more intuitive, because demonstrating to the Agent how it should perform and action should yield faster initial results compared to having it to learn from scratch by trial-and-error like Reinforced Learning. The first IL algorithm in the application is the Generative Adversarial Imitation Learning (GAIL) algorithm. The way the algorithm works is by having a second neural network, called the discriminator, be taught to distinguish whether a policy came from the demonstration given by a real player, or if the policy was produced by the agent. Essentially the goal of the agent is to trick the discriminator into thinking that the results were produced from the demo, not from the agent. At each training step the agent tries to maximize the reward, while the discriminator is trained to better distinguish between the demo and the agent, that in turn results in the agent getting better at copying the demo. The other IL algorithm is Behavioral Cloning (BC). As the name suggests this algorithm’s goal is to train the agent to copy the moves from the demo as close as possible.\nThe limitations of Imitation Learning is that they depend highly on the provided demo to do well in teaching the agent and they can not surpass said demo. That provides the agent a fast head start in learning how to do the given task. The limitation of Reinforced Learning is the opposite, it is slow to learn how to initially complete the task due to its trial-and-error approach, it may never even achieve its goal and learn “bad habits”, but unlike IL, RL does not have a limit on how good it can become at achieving the task, it can keep getting better. Thus the most optimal approach would be to combine the different algorithms and change the weight each has on the agent’s training at different stages. Imitation Learning and Reinforcement Learning supplement each other nicely.\nMethodology The application has been developed using Unity and the Unity ML Agents toolkit due to familiarity with the engine, the fast iteration time of using the engine and the ease of use and setup of the ML Agents Toolkit. The agent is set up using the Behavior Parameters script provided by the toolkit and a BeaconAgent script used to define the main logic loop for the agent, including how to move, information it needs to find the targets, and the rewards. On the Behavior Parameters component of the agent the only needed setup is Behavior Name, it is used by the configuration script to locate the agent. The first thing that is done in the Agent script is the OnEpisodeBegin() function. An episode is the singular training session defined by the toolkit, a single failed or successful loop. In the case of the application at the start of an episode the agent and button positions need to be reset to a new, random one, the goal needs to be reset and removed, and the button visuals need to be reset. It is important to note that all positions for objects use localPosition, that is the case in order to support Simultaneous Single-Agent training models. That allows the environments and agents to be reset in their local bounds. Next in the Behavior Parameters component is the Vector Observation field, which is used to tell the agent the information it needs in order to orient itself in the world. In the case of the application that data contains the state of the button, if it can be used or not, the direction to the button, the state of the goal, if it has been spawned or not, and if the goal has been spawned, then the agent also receives the direction towards the goal, if it has not, the agent just receives zeros. In total there are 6 observations necessary for the agent, hence the Space Size field in the component is set to 6. Next is the OnActionReceived() function. It is responsible for giving meaning to the agent inputs with the game world. The agent actions are numerical values which then need to be associated with the values and inputs needed for the agent to accomplish the task. The agent uses Discrete Actions, the values are whole numbers, compared to Continuous Actions which are floats and are constrained from -1 to 1. Essentially the agent uses the discrete actions as booleans in the case of the application. The first action is reserved for the left/right movement state of the agent, a value of 1 would mean that it should move to the left, a value of 2 would mean that it should move to the right and a value of 0 means dont move. The second action is reserved for forwards/backwards movement of the agent similar to the first action. Based on the value of the actions, the agent’s velocity is modified. The third and final action is used for the “Interact” button. The agent is given a reward if it successfully uses the button. The agent also receives a negative reward based on the amount of actions it has performed, thus encouraging the agent to complete the tasks with the least amount of actions. The next function is Heuristic, which is a state in which a player takes control of the agent and the AI is not in the learning process. Heuristics are mainly used for testing of the environment before allowing the agent to start training. The final function, OnTriggerEnter() is used for collision detection, when the agent successfully reaches the goal, it is given a reward and the current training episode ends, if the agent touches a wall, it is given a negative reward and also ends the training episode.\nThe last setup needed in order to start training the agent is a configuration file. It uses the yaml file format. The most important parts for this application are the reward_signals and the behavioral_cloning parameters. There are two types of reward signals. The first is extrinsic, that is responsible for rewards to work and essentially enable Reinforced Learning, the two required parameters are strength and gamma. Strength is the factor by which to multiply the reward given by the environment and ranges from 0 to 1. Gamma is responsible for deciding how far into the future the agent should care about possible rewards. In situations when the agent should be acting in the present in order to prepare for rewards in the distant future, this value should be large. In cases when rewards are more immediate, it can be smaller. Gamme needs to be smaller than 1, so for the application it is always set to 0.99 due to the nature of the test environment. The other parameter in reward_signals is “gail”, adding it to the config enables the gail algorithm to be used, it requires a strength parameter as well as a demo_path. The behavioral_cloning parameter is put outside of the reward_signals. Like gail it also requires a strength and demo_path parameters. With all of that setup, the application can be ran and the agent will start learning based on the configuration file. The demo used to train GAIL and Behavioral cloning agents was recorded based on 100 episodes with an average cumulative reward of 1.9, the highest score possible is 2.\nResults The Unity ML Agents Toolkit provides results which are nicely integrated into tensorboard. Two types of testing has been used. A single-agent scenario and simultaneous single-agent training. For the Single-agent training (figure 1), each agent was trained a hundred thousand episodes and the graphs are rated on the cumulative reward at every ten thousand sessions and the episode length, again at every ten thousand sessions. The Simultaneous single-agent training was done using 20 simultaneous agents for one million episodes (figure 2); they were rated on the same values as the single agent. There was also a test conducted that combined all 3 algorithms using a single-agent and trained for one hundred thousand episodes (figure 3).\nFigure 1 Note: Behavioural refers to behavioral cloning; Imitation refers to GAIL; Reinforced refers to Reinforced Learning Figure 2 Note: Behavioural refers to behavioral cloning; Imitation refers to GAIL; Reinforced refers to Reinforced Learning Figure 3 Note: Behavioural refers to behavioral cloning; Combined refers to all 3 algorithms together; Imitation refers to GAIL; Reinforced refers to Reinforced Learning Discussion of Results The results proved interesting. The single agent results showcase the theoretically expected results. Reinforced learning coming in last, with the worst cumulative score and highest amount of steps needed to complete the episode. That was expected due to the nature of reinforced learning being slow to pickup on more complex tasks due to its trial-and-error approach. What was observed from the brain at the end was that this agent never got to the goal, it only learned to reach and interact with the button, which was an expected side effect, it picking up a bad and undesirable habit. That could be fixed by itself with more data and training.\nA little surprising was that GAIL comes next in terms of cumulative score, not behavioral cloning. Due to the way the algorithm works it should be able to reach a better cumulative reward than behavioral cloning, due to the fact that it tries to improve a little past the demo, while behavioral can only get as good as the demo. That was most probably the case due to the small quantity of training, with more episodes, in theory, GAIL should provide better results over Behavioral Cloning. Another reason as to why BC has provided better early results could be to the fact that it does not have the overhead GAIL has, as in, it is trying to copy the demo exactly, while GAIL is trying to outsmart the discriminator, which takes more time to learn to do effectively.\nUnsurprisingly combining all of the algorithms provided the best result in terms of cumulative reward. It is noteworthy that for the configuration of the combined learning the strength for the reinforced learning was set to 1, while both GAIL and BC were set to 0.5, this the agent prioritized reinforcement learning while benefiting from the headstart imitation learning techniques provided.\nAnother interesting result was that GAIL was able to complete the episode with the least amount of steps, compared to the other algorithms. Important to note is that episode length can greatly be affected by failed attempts as well, which count as low-step episodes.\nThe results from the simultaneous single agent tests are interesting. They completely contradict the results from the smaller testing sample, they also contradict the expected theoretical results. Reinforcement Learning came out on top by an undeniable margin, performing almost perfectly, while both GAIL and BC struggled to maintain a positive cumulative reward. These results were confirmed by re-doing them 3 times, each result being the same. The theory why GAIL and BC struggled as much is due to performance limitations and the train the ML took on the hardware. Although for that to be true, RL should have had similar results to both GAIL and BC. When viewing the results in the application using the generated brain, the reinforcement learning agent undoubtedly flies through the task and episode, while both BC and GAIL manage to pass it much like how an ogre would stumble through, they pass it, but not very gracefully not fast. Given enough training, it is expected that RL will provide better results than both BC and GAIL, but the graph shows that those differences happened extremely early in the testing and the gap barely closed. Considering the demo used for GAIL and BC had a cumulative rewards score of 1.9, in theory BC should have been able to get much closer to that score.\nConclusion In single agent testing episodes, all of the algorithms performed as expected. The only unexpected behavior was observed in the simultaneous single agent testing. In order to explore the reasons for the results better hardware should be used. The application test were conducted on a laptop with an i7-8750H CPU and GTX1060 max-q GPU. During testing, the simultaneous single agent application was running at ~4fps, during the testing of the single-agent, the application was running at ~15fps. The truth is that Machine Learning is computationally expensive. The application itself uses small assets and is very lightweight, when ran without the ML agent training on top, the application ran at ~120fps. Even for single agent training ML took its toll and pushed the hardware to its limits. Another proof of how far the application was pushed was that for most of the duration of the tests, the CPU/GPU temperatures recorded were in the range of 92-95 degrees Celsius.\nOverall I am happy with the results and knowledge gained, it is a good basis for future exploration in the field and training more complex AI. Main takeaway is that the training should probably be done in smaller quantities but longer duration, given the hardware. The training can also be conducted in shorter, but multiple training sessions instead of a single hours long session.\n","description":"Exploring ML using the Unity ML-Agents Toolkit","id":0,"section":"posts","tags":["AI","ML","c_sharp"],"title":"AI using Unity ML-Agents","uri":"https://devpilgrim.com/posts/ai_mlagents_unity/"},{"content":"Summary The mechanics I have implemented is a player resource (Health/Mana) management and\nmanipulation mechanic in similar fashion to MMORPG/ARPG/MOBA etc games. Diablo\nand Path of Exile served as inspiration. The mechanic is made up of several components:\n  Health system, where:\n Taking damage causes the player to lose HP Picking up HP Potions recovers Player HP Easy access to HP values    Mana system, where:\n Spells cost mana to cast Spells cost mana to channel Mana regenerates over time Picking up Mana Potions recovers a flat Mana amount Picking up Mana Regen Buffs increases mana recovery rate drastically for a limited period of time Spells can only be cast if the Player has enough Mana    Spell Cooldowns:\n Single Cast Spells can only be cast while not on Cooldown Channeling Spells do not have a Cooldown (although some games in the genre implement a Cooldown for some Channeling Spells and this can easily be added here as well)    Health/Mana Orbs UI:\n Used showcase current and max Player Health and Mana resources    Multiple abilities/spells that can be Cast:\n “Vortex” - a channeling ability which deals steady damage over time to enemies in it’s area of effect. Has an Initial Mana Cost associated with it on first use, later has a relatively small and fixed channeling cost/ “Rock Blast” - a single cast ability with high base damage, high mana cost and high cooldown.    Pickups:\n Health Potion which restores a flat amount of HP to the Player when picked up Mana Potion which restores a flat amount of Mana to the Player when picked up Mana Regen Buff which greatly increases Player Mana Regen for a short period Explosive Bomb which damages and subtracts a flat amount of HP from the Player on contact    Enemy Dummy\n Has a detection radius for the player, where if the player is inside the enemy starts chasing the player Has an attack radius, where if the player is inside the enemy starts attacking the player  Attack has a separate hitbox   Health Death conditions    ","description":"Gameplay Mechanics implementation in Unreal - University Coursework","id":1,"section":"posts","tags":["mechanic","cpp"],"title":"AI using Unity ML-Agents","uri":"https://devpilgrim.com/posts/gameplaymechanic_unreal/"},{"content":"For this module we had to network a game using any networking API. During the classes we were taught winsock and SFML. For my coursework I decided to use my year 1 asteroids game as a base and add networking using SFML on top.\nThe Network Architecture I chose for the projcet was Server-Client. Reasoning being that it scales easier than P2P. It is more secure, because the Server has to verify the data from the players and more stable if a Client crashes, thus not affecting the other Clients if the host-client crashes by mistake in P2P. Server can currently handle up to 4 clients, but the game is limited to 2 sprites to make testing faster.\nFor the Transport layer protocols I chose TCP for handling the player Lives, The total Score, each client\u0026rsquo;s ID and the game Time. Those are things which I do not want to lose track of in case of Packet drops. For the player positions, rotation and packet sent time I used UDP. A packet not arriving containing that data does not impact the game there is\ninterpolation for the player positions.\nApplication layer Protocols Client (every packet is cleared after being received/sent):  Receives TCP Init packet on first connection with the server containing int values for Life/Score/ClientID Receives TCP packet containing Lives/Score/Server GameTime Receives UDP Packet containing player position/rotation/time when other client sent packet and pushes them into separate vectors for later use with interpolation Sends UDP packet every 1/3s containing player position/rotation/Client game time Sends TCP packet every 1/3s containing Lives and Score  Server (every packet is cleared after being received/sent):  Establishes a Listener and a vector of sf::TcpSocket* On first connection with client accept *socket and add socket to vector If new client sends TCP Init packet containing int values for Life/Score/ClientID On existing TCP clients sends packet containing Lives/Score/Server GameTime every 1/3s On existing UDP clients receive packet containing player position/rotation variables (floats) and a float containing when the client sent the packet On UDP receive add the Port of the sender into an std::set On UDP send go through std::set of ports and send packet to everyone but the original sender  Network Code Structure:  Non-blocking sockets for TCP and UDP past the initial connection Server.exe handles both TCP and UDP communication and clients   TCP communication is handled through a vector containing all connected client sockets     UDP communication is handled through an std::set containing all the ports the server has received a UDP packet from   Client.exe handles sending and receiving information given to it by the server  Lessons Learned  Newfound appreciation for network programmers Need to work on my network architecture skills Need to start from a better organised game and work up. Handling multiple clients can cause time desync between clients - need a very good time handling method/architecture A lot about timing and clocks in games (which was also the origin of the majority of the issues I had)  ","description":"First Networking Application I made, used the Asteroids game as a base","id":2,"section":"posts","tags":["sfml","networking","cpp"],"title":"Networking with Asteroids","uri":"https://devpilgrim.com/posts/asteroids_networked/"},{"content":"Hello and welcome back In this post I will go over how I\u0026rsquo;ve implemented object pooling into a simple game I made in Unity. I have been wanting to write this for a while, but I was on holiday and then had to catch up on work and did not have the time to do so. So without further adieu let\u0026rsquo;s dive in.\nLet\u0026rsquo;s start with understanding what an object pool is. For example in the case of Unity whenever you want to create an enemy ship you call Instantiate() and when you want to destroy it you call Destroy(). Those are helpful functions during gameplay and used quite often, they also generally require minimal CPU time.\nHowever, for objects created during gameplay that have a short lifespan and get destroyed in vast numbers per second, the CPU needs to allocate considerably more time. A great example for that is bullets. They are often on the screen for a fraction of a second and there\u0026rsquo;s tens or hundreds of them at any given moment. Constantly having to call Instantiate() and Destroy() can get quite resource intensive. Also repeatedly calling Destroy() frequently trigger Unity\u0026rsquo;s Garbage Collector which further slows down the CPU and may introduce pauses to gameplay.\nObject pooling is where you pre-instantiate all the objects you’ll need at any specific moment before gameplay — for instance, during a loading screen. Instead of creating new objects and destroying old ones during gameplay, your game reuses objects from a “pool”.\nAnother option, tho simpler, is to load all the game objects you wish to use off-screen in an array and disable their logic. When you want to spawn an enemy just take the first disabled enemy and position him to where you want him to be spawned and enable his logic. That is a less scaleable and more simplistic form of an Object Pool using just an array.\nSo how did I go about making my object pooler? First I created a new script in Unity duuh and started by making the script a singleton. Several other scripts will need access to the object pool during gameplay and the singleton allows other scripts to access it without getting a Component from a GameObject.\n1 2 3 4  public static ObjectPooler Instance; void Awake() { Instance = this; }   Next I add using System.Collection.Generic. I will be using a generic list to store my pooled objects. Typically, you use generics when working with collections. This approach allows you to have an array that only allows one type of object, preventing you from putting a dog inside a cat array.\nThen I add create the Object pool List along with 2 new variables. You can also make them private and Serialize the fields so that you can access them in the Unity Inspector.\n1 2 3  public List\u0026lt;GameObject\u0026gt; pooledObjects; public int amountToPool; public GameObject objectToPool;   After we have the List and variables setup it\u0026rsquo;s time to instantiate the GameObject objectToPool. I do that in the Start()function with a for loop based on the number of times in the int amountToPool variable I created earlier. Then I set the GameObjects to inactive before adding them to the pooledObjects list.\n1 2 3 4 5 6 7 8  pooledObjects = new List\u0026lt;GameObject\u0026gt;(); for (int i = 0; i \u0026lt; item.amountToPool; i++) { GameObject obj = (GameObject)Instantiate(item.objectToPool); obj.SetActive(false); pooledObjects.Add(obj); }   At this point when you run the game in the hierarchy you will see all of the pooled objects based on the amount you specified in the Inspector.\nNext we want to be able to get an object from the object pool.\n1 2 3 4 5 6 7 8 9  public GameObject GetPooledObject() { for (int i = 0; i \u0026lt; pooledObjects.Count; i++) { if (!pooledObjects[i].activeInHierarchy) { return pooledObjects[i]; } } return null; }   It is important to note that the GetPooledObject()function has a return type of GameObject. First we iterate throught the pooledObjects list, then check to see if the item in the list is not currently active in the Scene. If it is, the loop moves to the next object in the list. If not, then exit the function and give the inactive object to the function that called GetPooledObject(). Now that I have a way to get an item from the object pool I need to replace all of the Instantiate() and Destroy() code so that we can start using it.\nI instantiate my bullets in the Player class using a Fire() function. The function is called during Update() when the Mouse Button is held down.\nThe important thing to notice is that I ask the Fire() function to ask the ObjectPooler for a pooled object. If one is available, it\u0026rsquo;s set to the desired position and then set to active.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  private void Fire() { Vector2 direction = mousePos - myRigidBody.position; direction.Normalize(); GameObject bullet = ObjectPooler.Instance.GetPooledObjects(\u0026#34;PlayerBullet\u0026#34;); if (bullet != null) { bullet.transform.position = bulletFirePos.position; bullet.SetActive(true); } else { return; } Rigidbody2D bulletBody = bullet.GetComponent\u0026lt;Rigidbody2D\u0026gt;(); bulletBody.rotation = GetZAngle(0); bulletBody.velocity = direction * bulletSpeed; }   Next step is to handle and replace the Destroy() functionality of the bullets. The way I\u0026rsquo;ve handled that is quite simple. I have setup 4 colliders around my screen which are also Triggers. When a bullet intersects with a collider the OnTriggerEnter2D() function is called and I set the bullet to inactive. Doing the same thing if the bullet and an enemy collide as well.\n1 2 3 4 5 6 7 8 9 10 11  void OnTriggerEnter2D(Collider2D other) { bool isShredder = other.CompareTag(\u0026#34;Shredder\u0026#34;); if (isShredder) { gameObject.SetActive(false); } bool isEnemy = other.CompareTag(\u0026#34;EnemyEasy\u0026#34;); if (isEnemy) { gameObject.SetActive(false); } }   Now when I hit play all the hard work pays off and the object pooler works wonderfully. All that is left is to figure out how big we want the List to be. If I\u0026rsquo;m ever going to have 10 bullets on screen at the same time, it would be pointless to have the list instantiate 100 bullets.\nOf course that raises the question. What happens if we somehow end up with 11 bullets when the List only supports 10? Luckily it is quite easy to add the functionality to expand our List.\nFirst create a new public bool shouldExpand = true;. That will add a checkbox in the Inspector where we can indicate whether or not we want the List to be able to expand.\nNext in the GetPooledObjects() just replace the return null; with:\n1 2 3 4 5 6 7 8  if (shouldExpand) { GameObject obj = (GameObject)Instantiate(objectToPool); obj.SetActive(false); pooledObjects.Add(obj); return obj; } else { return null; }   If a new bullet is requested from the pool but no inactive ones can be found, this if statement checks to see if it\u0026rsquo;s possible to expand the pool instead of exiting the function. If it is, a new bullet is instantiated and added to the pool, also sets it to inactive and is returned to the function that called GetPooledObjects().\nNow we finally have a simple and fully working Object Pooler. There is plenty of ways I could expand it in the future. Such as adding the functionality to have multiple lists for enemies, explosions, powerups etc. For now this works perfectly for my small project. Hope you enjoyed reading through this blog post and learned something new.\nSee you again in the next post! ","description":"How I implemented Object Pooling in a side project in Unity","id":3,"section":"posts","tags":["unity","mechanic","c_sharp"],"title":"Object Pooling - Deep Dive","uri":"https://devpilgrim.com/posts/objectpooling/"},{"content":"This was the very first game I made when I got into University. It was made as a project for the CMP105 Games Course (Link for the module description below).\nThe game was made utilizing the SFML Library and C++.\nAs my very first game I learned quite a bit, and I also messed up quite a lot. It was a good introduction into the of games programming which is also what sparked my interest to continue learning and wanting to get better at it.\nEvery project is a stepping stone onto and this project was my first step into the world of game programming.\n","description":"First Game I ever made","id":4,"section":"posts","tags":["sfml","cpp"],"title":"Asteroids","uri":"https://devpilgrim.com/posts/asteroids/"},{"content":"With this course we got introduced to OpenGL, or more specifically GLUT. The goal of this course and project was to design and develop a 3D graphics application and scene that exhibits key techniques in graphics programming using OpenGL.\nThe techniques I have successfuly implemented are:\nLighting  Point light and a spot light different properties on the geometry to simulate different reflection values and materials  Geometry  hand crafted geometry using vertexes transparent textures depth buffer for the skybox hierarchy modelling for planets orbiting a sun trilinear filtering for the ground texture  Camera  fully functioning 1st person camera with keyboard and mouse movement static birds eye camera\n  This was an amazing learning experience. I had not used an API before and it was interesting to learn how the render pipeline works. The biggest challenge I faced in this project was the calculations for the camera, but with the help of the teaching staff and fellow students I got it working as I wanted and understood much better how the math behind it works.\n","description":"First experience using OpenGL and the Render Pipeline","id":5,"section":"posts","tags":["opengl","cpp"],"title":"OpenGL Scene","uri":"https://devpilgrim.com/posts/opengl/"},{"content":"Year 2 we started learning about different kinds of data structures and algorithms as well as applying them.\nFor the assessment project we had to create an application which implements two different standard algorithms with the appropriate data structures and the ability to vary the size of the input data.\nI chose two different string searching algorithms. The Rabin Karp algorithm which is utilizes a brute force process and the Boyer Moore algorithm, which utilizes a skipping approach to string searching.\nThe performance of the algorithms was compared using the following inputs in an abstract from the book \u0026ldquo;Lord of the Rings\u0026rdquo;:\n most common english words most common english letter least common english letter string that is not contained in the text string at the end of the document very long string that is contained in the text\n  Testing with both algoritms I came to the conclusion that the Boyer Moore algorithm excels when it has to skip more in between found strings. When both algorithms are searching for a common string inside the text, the performance difference is close between both algorithms. If you are searching for something more “rare” or less likely to be contained in the text, then use Boyer Moore.\n","description":"Comparing Boyer-Moore vs Rabin Karp","id":6,"section":"posts","tags":["algorithms","cpp"],"title":"String Searching - Data Structures and Algorithms","uri":"https://devpilgrim.com/posts/string_searching/"},{"content":"This was my last project for Year 2. Purpose of the project is to demonstrate knowledge and proficiency by using and manipulating textured 3D geometry, multiple interactable game states and physics interactions using the Box2D library (3D objects on a 2D plane) and the GEF framework.\nGameplay wise you have a player ship and respawnable enemies. The goal is to survive for as long as possible while getting the highest score you can. There are collision based physics interactions between the player, enemies, bullets and ground. A spin I added is that you can manipulate the bullets while they are flying. You as the player decide in which direction they fly and it can be changes mid air.\nThe biggest thing I learned from this project was how to make my own class based State Machine. Apart from the state machine was interesting to use the Box2D library for the first time and not have to write my own collision.\n","description":"My first experience with Box2D and my last project for Year 2","id":7,"section":"posts","tags":["gef","box2d","cpp"],"title":"2.5D Game using Box2D and the GEF Framework","uri":"https://devpilgrim.com/posts/gef_and_box2d/"},{"content":"Sample images from Pixabay\n","description":"cartoon gallery","id":8,"section":"gallery","tags":null,"title":"Cartoon","uri":"https://devpilgrim.com/gallery/cartoon/"},{"content":"Sample images from Pixabay\n","description":"photo gallery","id":9,"section":"gallery","tags":null,"title":"Photo","uri":"https://devpilgrim.com/gallery/photo/"},{"content":"My name is Hristo and I am a final year Computer Games Application Development student at Abertay University. In this website you can find a combination of my Portfolio as well as blog posts I write. \rSince a very young age I have been playing video games. The more I played the more interested I became in how they are made and wanted to learn how to make them. This is reason why after graduation High School I enrolled into Abertay University to study Computer Game Application Development. The more I study about games and how they are made the more passionate I become about it. It is my dream job and am constantly working on new side projects to further my skills and knowledge in the industry.\n","description":"Description","id":11,"section":"","tags":null,"title":"About","uri":"https://devpilgrim.com/about/"}]